{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Marathon Training Application Story Template\n",
    "##### Objective: Create an engaging story template for a marathon training application that adapts to user prompts, running metrics, and training styles. This story template will be used to generate specific stories based on user inputs and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "import torch\n",
    "import evaluate\n",
    "from utils import save_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPT-7B Instruct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "C:\\Users\\pmahi\\.cache\\huggingface\\modules\\transformers_modules\\mosaicml\\mpt-7b-storywriter\\b41e79e5f836d9b49637779cb37db32a965c3047\\configuration_mpt.py:114: UserWarning: alibi or rope is turned on, setting `learned_pos_emb` to `False.`\n",
      "  warnings.warn(f'alibi or rope is turned on, setting `learned_pos_emb` to `False.`')\n",
      "C:\\Users\\pmahi\\.cache\\huggingface\\modules\\transformers_modules\\mosaicml\\mpt-7b-storywriter\\b41e79e5f836d9b49637779cb37db32a965c3047\\configuration_mpt.py:141: UserWarning: If not using a Prefix Language Model, we recommend setting \"attn_impl\" to \"flash\" instead of \"triton\".\n",
      "  warnings.warn(UserWarning('If not using a Prefix Language Model, we recommend setting \"attn_impl\" to \"flash\" instead of \"triton\".'))\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7216c0fa8b1e41a79daab6447b064c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "model_id =\"mosaicml/mpt-7b-storywriter\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\" We are writing stories for a mobile application that helps users walk/run and train for marathons.  \n",
    "The genre and description will be in Prompt.\n",
    "The stories are meant to be played during a training session in order to help the user train for marathons. \n",
    "The story should have embedded cues for speed changes and breaks where approriate depending on the Training Style.\n",
    "\\n\n",
    "\n",
    "Can you help me write a story with the following details:\n",
    "Prompt: A zombie horror story set in New York City.\n",
    "Distance: 0.5 miles\n",
    "Average Walking Speed: 2 mph\n",
    "Average Running Speed: 5 mph\n",
    "Training Style: Short-distance sprint \\n\n",
    "\"\"\"\n",
    "# print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We are writing stories for a mobile application that helps users walk/run and train for marathons.  \n",
      "The genre and description will be in Prompt.\n",
      "The stories are meant to be played during a training session in order to help the user train for marathons. \n",
      "The story should have embedded cues for speed changes and breaks where approriate depending on the Training Style.\n",
      "\n",
      "\n",
      "\n",
      "Can you help me write a story with the following details:\n",
      "Prompt: A zombie horror story set in New York City.\n",
      "Distance: 0.5 miles\n",
      "Average Walking Speed: 2 mph\n",
      "Average Running Speed: 5 mph\n",
      "Training Style: Short-distance sprint \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pmahi\\OneDrive\\Desktop\\MP\\Drexel\\Courses\\CS614-900 - Applications in Machine Learning\\Projects\\Natural_Language_Assignment\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pmahi\\.cache\\huggingface\\modules\\transformers_modules\\mosaicml\\mpt-7b-storywriter\\b41e79e5f836d9b49637779cb37db32a965c3047\\attention.py:87: UserWarning: Propagating key_padding_mask to the attention module and applying it within the attention module can cause unnecessary computation/memory usage. Consider integrating into attn_bias once and passing that to each attention module instead.\n",
      "  warnings.warn('Propagating key_padding_mask to the attention module ' + 'and applying it within the attention module can cause ' + 'unnecessary computation/memory usage. Consider integrating ' + 'into attn_bias once and passing that to each attention ' + 'module instead.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|># **THE** _BOOK OF_   **_DARKNESS_**\n",
      "\n",
      " © 2012 by John Connolly, Ltd., Dublin 1 Ireland \n",
      " All rights reserved under International Copyright Conventions. By payment of required fees, You may not copy or distribute this book without written permission from The publisher (except as provided below). No part o fthis publication ma ybe reproduced e lsewh en t he p ermissionof th is copyright owner whic h has been obtained through permissions service s at www dot permis sonline netw ook com /permissions/. For information about photocopying an d other reprographi c reproduction righ ts contact us via our web site : http://www wileycom reprintsandpermissionsofworksbyjohnconnolloyltd iee mailtojclreprintsonly@ukpenguingroup co uk This edition published simultaneously i n Great Britain Australia Canada France Germany India Japan Mexico Spain Taiwan South Korea Singapore Thailand Turkey Vietnam & USA Wiley Publishing Asia Pte Limited 37th Level 731A North Bridge Road West Hong Kong Tel +852 3 1241 6100 Fax 852 2521 9009 ISBN 9781118050114 Printedin China Contents Preface ix Chapter One xiii Part I - THE BLOODSTONE CHRONICLES CHAPTER ONE \\- DANTE'S STORY xxv PART II – ANGELICA AND HER SON xxxiv chapter two — angelica's Story XXXIX chapteer three—the Bloodstone Chronicles XLVIIIV Chaper four–angelika Returns XLVIXXI XXVIII Angelique And Her Son Continues... LXXXVIIXLIXLXVLLLXIXXXX VOLUME TWO CONTINUES.... XVIL Volume Three Begins... LVILL VoluMe Fourteen Ends...... LIxxiiiixxxviixxxvixxxxxivxlxxxxxxxxliillllllllvolumn Fifteeen Starts....... lixcChapter Sixtyone --- Dante Meets His Father Again! Lixthe End Of An Era!!!!!! LLlxlivoxcEpilo gue To \"Darkness\" From Its Author Llxfifteenth Anniversary Edition Dedication llffourrdeventeenth anniversary ed ition ded ication iiie fifthteen anniversarry edit ion dedication iiiietwooetwentytenthannive rydediction ivielittleblackdickensdayeditio nd viernineteeenth\n"
     ]
    }
   ],
   "source": [
    "hyperparams = {\n",
    "    \"max_new_tokens\": 512, \n",
    "    \"temperature\":0.6, \n",
    "    \"repetition_penalty\":2.0, \n",
    "    \"no_repeat_ngram_size\":1\n",
    "}\n",
    "encoded_input = tokenizer(input_text, return_tensors='pt').to(\"cuda\")\n",
    "streamer = TextStreamer(tokenizer)\n",
    "response = model.generate(input_ids=encoded_input.input_ids, \n",
    "                        streamer=streamer, \n",
    "                        max_new_tokens=hyperparams[\"max_new_tokens\"], \n",
    "                        temperature=hyperparams[\"temperature\"], \n",
    "                        repetition_penalty=hyperparams[\"repetition_penalty\"], \n",
    "                        no_repeat_ngram_size=hyperparams[\"no_repeat_ngram_size\"]).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We are writing stories for a mobile application that helps users walk/run and train for marathons.  \n",
      "The genre and description will be in Prompt.\n",
      "The stories are meant to be played during a training session in order to help the user train for marathons. \n",
      "The story should have embedded cues for speed changes and breaks where approriate depending on the Training Style.\n",
      "\n",
      "\n",
      "\n",
      "Can you help me write a story with the following details:\n",
      "Prompt: A zombie horror story set in New York City.\n",
      "Distance: 0.5 miles\n",
      "Average Walking Speed: 2 mph\n",
      "Average Running Speed: 5 mph\n",
      "Training Style: Short-distance sprint \n",
      "\n",
      "# **THE** _BOOK OF_   **_DARKNESS_**\n",
      "\n",
      " © 2012 by John Connolly, Ltd., Dublin 1 Ireland \n",
      " All rights reserved under International Copyright Conventions. By payment of required fees, You may not copy or distribute this book without written permission from The publisher (except as provided below). No part o fthis publication ma ybe reproduced e lsewh en t he p ermissionof th is copyright owner whic h has been obtained through permissions service s at www dot permis sonline netw ook com /permissions/. For information about photocopying an d other reprographi c reproduction righ ts contact us via our web site : http://www wileycom reprintsandpermissionsofworksbyjohnconnolloyltd iee mailtojclreprintsonly@ukpenguingroup co uk This edition published simultaneously i n Great Britain Australia Canada France Germany India Japan Mexico Spain Taiwan South Korea Singapore Thailand Turkey Vietnam & USA Wiley Publishing Asia Pte Limited 37th Level 731A North Bridge Road West Hong Kong Tel +852 3 1241 6100 Fax 852 2521 9009 ISBN 9781118050114 Printedin China Contents Preface ix Chapter One xiii Part I - THE BLOODSTONE CHRONICLES CHAPTER ONE \\- DANTE'S STORY xxv PART II – ANGELICA AND HER SON xxxiv chapter two — angelica's Story XXXIX chapteer three—the Bloodstone Chronicles XLVIIIV Chaper four–angelika Returns XLVIXXI XXVIII Angelique And Her Son Continues... LXXXVIIXLIXLXVLLLXIXXXX VOLUME TWO CONTINUES.... XVIL Volume Three Begins... LVILL VoluMe Fourteen Ends...... LIxxiiiixxxviixxxvixxxxxivxlxxxxxxxxliillllllllvolumn Fifteeen Starts....... lixcChapter Sixtyone --- Dante Meets His Father Again! Lixthe End Of An Era!!!!!! LLlxlivoxcEpilo gue To \"Darkness\" From Its Author Llxfifteenth Anniversary Edition Dedication llffourrdeventeenth anniversary ed ition ded ication iiie fifthteen anniversarry edit ion dedication iiiietwooetwentytenthannive rydediction ivielittleblackdickensdayeditio nd viernineteeenth\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_response(\"MPT-7B-STORYWRITER\", hyperparams, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "# input_texts = [\"lorem ipsum\", \"Happy Birthday!\", \"Bienvenue\"]\n",
    "# results = perplexity.compute(model_id=model_id,\n",
    "#                              add_start_token=False,\n",
    "#                              predictions=input_texts, \n",
    "#                              device=\"cuda\")\n",
    "# print(list(results.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(round(results[\"mean_perplexity\"], 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
