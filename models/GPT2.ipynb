{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Marathon Training Application Story Template\n",
    "##### Objective: Create an engaging story template for a marathon training application that adapts to user prompts, running metrics, and training styles. This story template will be used to generate specific stories based on user inputs and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "import torch\n",
    "import evaluate\n",
    "from utils import save_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 Instruct Model\n",
    "\n",
    "GPT2-XL SFT on Deita dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af71be38308a49728ff1577b6aab8883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id =\"KnutJaegersberg/gpt2-chatbot\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"### System:\\n\n",
    "You are a AI story writer who creates stories in various genres. We are writing stories for a mobile application that helps users walk/run and train for marathons.  \n",
    "The genre and description will come by user input called Prompt.\n",
    "The stories are meant to be played during a training session in order to help the user train for marathons. \n",
    "The story should have embedded cues for speed changes and breaks where approriate depending on the Training Style.\n",
    "\\n\n",
    "### User: \\n\n",
    "\n",
    "Can you help me write a story with the following details:\n",
    "Prompt: A zombie horror story set in New York City.\n",
    "Distance: 0.5 miles\n",
    "Average Walking Speed: 2 mph\n",
    "Average Running Speed: 5 mph\n",
    "Training Style: Short-distance sprint \\n\n",
    "\n",
    "### AI Story Writer:\\n\\n\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### System:\n",
      "\n",
      "You are a AI story writer who creates stories in various genres. We are writing stories for a mobile application that helps users walk/run and train for marathons.  \n",
      "The genre and description will come by user input called Prompt.\n",
      "The stories are meant to be played during a training session in order to help the user train for marathons. \n",
      "The story should have embedded cues for speed changes and breaks where approriate depending on the Training Style.\n",
      "\n",
      "\n",
      "### User: \n",
      "\n",
      "\n",
      "Can you help me write a story with the following details:\n",
      "Prompt: A zombie horror story set in New York City.\n",
      "Distance: 0.5 miles\n",
      "Average Walking Speed: 2 mph\n",
      "Average Running Speed: 5 mph\n",
      "Training Style: Short-distance sprint \n",
      "\n",
      "\n",
      "### AI Story Writer:\n",
      "\n",
      "\n",
      "Sure, here's an example of how I might approach this prompt using Python code (assuming your language is not English):```python from sklearn import linear_model as lm # Load data into XGBoost model x = np. random() y1,...yN - 1 \\* len(x) + numpy([np[i] * 100]) / 255 def predict\\_(self){ return self['target'] if 'Target' == str('X', target='train') else None } print(\"Predicted distance between \"..strftime($f\"{len(_))}\")[\"Train\"]+\": \",predictions=lmb().fitna()) predictions=\"\",outputs={},verbose=[0]} output2=(input3).toarray(), outputs=$outfile$OutputFileName $OutDir/$InputFilename [download all files](https://githubusercontent...tify?raw=-qwvYjZz4JU&revisionNumber=/Users%40paulkroppenski.... )# Download CSV file containing distances predicted per km or miles respectively prededata=\"/path/.csv\".splitlines(-8)[:-6].drop(('.dic').replace(\\*.txt,\"\").join),dfidr=~/(^|\\\\.)/,maxlength~10 dfids=\"\" fname=\"#D:\\Data Files\\\" dir=\\\"/\" filename=#FNAME## csvdir:/home//Downloading Data File...\" downloader <- pymongo::curl({uri:\"http:,host\":\"localhost\"})/data; headers+={\"ContentType\":{\"text\\\":json}} response { contenttype : json }; header lines {\"--->> Distance Prediction\",\"DateTimeOffset\":-14400000}\" line 3 [\"--->\" ] endline \"\\n\"; // Get list length at each row foreach rows % 10 do |rownum,-int64-| begin result := [] while true loop select max(*rows)-min (*width)+sum(&result); break unless @error &&!@endforever &!isempty (%total)\\b done until total > width limit 1000 ; next rnd -> int32 => start index i j k h -- --> ResultLength :: Int16 count range min avg maximum ----------> --- ------ --------- ------- ----------------------------- ---------------------------------- ---- ----- ------------------------------------------------------+-----------------------------+----------------------- 00010000 0100000 02100010 03200000 04000000 050000000 0620000 07300110 08500120 09700150 10000 110 200 300\n"
     ]
    }
   ],
   "source": [
    "hyperparams = {\n",
    "    \"max_new_tokens\": 512, \n",
    "    \"temperature\":0.6, \n",
    "    \"repetition_penalty\":2.0, \n",
    "    \"no_repeat_ngram_size\":1\n",
    "}\n",
    "encoded_input = tokenizer(input_text, return_tensors='pt').to(\"cuda\")\n",
    "streamer = TextStreamer(tokenizer)\n",
    "response = model.generate(input_ids=encoded_input.input_ids, \n",
    "                        streamer=streamer, \n",
    "                        max_new_tokens=hyperparams[\"max_new_tokens\"], \n",
    "                        temperature=hyperparams[\"temperature\"], \n",
    "                        repetition_penalty=hyperparams[\"repetition_penalty\"], \n",
    "                        no_repeat_ngram_size=hyperparams[\"no_repeat_ngram_size\"]).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### System:\n",
      "\n",
      "You are a AI story writer who creates stories in various genres. We are writing stories for a mobile application that helps users walk/run and train for marathons.  \n",
      "The genre and description will come by user input called Prompt.\n",
      "The stories are meant to be played during a training session in order to help the user train for marathons. \n",
      "The story should have embedded cues for speed changes and breaks where approriate depending on the Training Style.\n",
      "\n",
      "\n",
      "### User: \n",
      "\n",
      "\n",
      "Can you help me write a story with the following details:\n",
      "Prompt: A zombie horror story set in New York City.\n",
      "Distance: 0.5 miles\n",
      "Average Walking Speed: 2 mph\n",
      "Average Running Speed: 5 mph\n",
      "Training Style: Short-distance sprint \n",
      "\n",
      "\n",
      "### AI Story Writer:\n",
      "\n",
      "\n",
      "Sure, here's an example of how I might approach this prompt using Python code (assuming your language is not English):```python from sklearn import linear_model as lm # Load data into XGBoost model x = np. random() y1,...yN - 1 \\* len(x) + numpy([np[i] * 100]) / 255 def predict\\_(self){ return self['target'] if 'Target' == str('X', target='train') else None } print(\"Predicted distance between \"..strftime($f\"{len(_))}\")[\"Train\"]+\": \",predictions=lmb().fitna()) predictions=\"\",outputs={},verbose=[0]} output2=(input3).toarray(), outputs=$outfile$OutputFileName $OutDir/$InputFilename [download all files](https://githubusercontent...tify?raw=-qwvYjZz4JU&revisionNumber=/Users%40paulkroppenski.... )# Download CSV file containing distances predicted per km or miles respectively prededata=\"/path/.csv\".splitlines(-8)[:-6].drop(('.dic').replace(\\*.txt,\"\").join),dfidr=~/(^|\\\\.)/,maxlength~10 dfids=\"\" fname=\"#D:\\Data Files\\\" dir=\\\"/\" filename=#FNAME## csvdir:/home//Downloading Data File...\" downloader <- pymongo::curl({uri:\"http:,host\":\"localhost\"})/data; headers+={\"ContentType\":{\"text\\\":json}} response { contenttype : json }; header lines {\"--->> Distance Prediction\",\"DateTimeOffset\":-14400000}\" line 3 [\"--->\" ] endline \"\\n\"; // Get list length at each row foreach rows % 10 do |rownum,-int64-| begin result := [] while true loop select max(*rows)-min (*width)+sum(&result); break unless @error &&!@endforever &!isempty (%total)\\b done until total > width limit 1000 ; next rnd -> int32 => start index i j k h -- --> ResultLength :: Int16 count range min avg maximum ----------> --- ------ --------- ------- ----------------------------- ---------------------------------- ---- ----- ------------------------------------------------------+-----------------------------+----------------------- 00010000 0100000 02100010 03200000 04000000 050000000 0620000 07300110 08500120 09700150 10000 110 200 300\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_response(\"GPT2\", hyperparams, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "# input_texts = [\"lorem ipsum\", \"Happy Birthday!\", \"Bienvenue\"]\n",
    "# results = perplexity.compute(model_id=model_id,\n",
    "#                              add_start_token=False,\n",
    "#                              predictions=input_texts, \n",
    "#                              device=\"cuda\")\n",
    "# print(list(results.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(round(results[\"mean_perplexity\"], 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
