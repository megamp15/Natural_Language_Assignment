{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6aa40db-d9a3-4b99-9d95-b44af2e83b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a8e4a85-4c13-40dd-9d1e-7a133719c73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Dear Amazon, last week I ordered an Optimus Prime action figure \\\n",
    "from your online store in Germany. Unfortunately, when I opened the package, \\\n",
    "I discovered to my horror that I had been sent an action figure of Megatron \\\n",
    "instead! As a lifelong enemy of the Decepticons, I hope you can understand my \\\n",
    "dilemma. To resolve the issue, I demand an exchange of Megatron for the \\\n",
    "Optimus Prime figure I ordered. Enclosed are copies of my records concerning \\\n",
    "this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf127a0a-98f3-4217-9519-6388e23162d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 6c0e608 (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "c:\\Users\\pmahi\\OneDrive\\Desktop\\MP\\Drexel\\Courses\\CS614-900 - Applications in Machine Learning\\Projects\\Natural_Language_Assignment\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "539545bf5733423fa0fc470f32a2c4d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d53e6a8b514b4158bf1809f93e799c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8522af03e9774b28815c53612f69b152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5fcdf6667f2405887e109343ae9f732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ba7168afdd4548b49cb002cc29631b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1603166f51c842a29023fca79c6888a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e629db91c59e487e81c8f9b0c1cdb0d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear Amazon, last week I ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead! As a lifelong enemy of the Decepticons, I hope you can understand my dilemma. To resolve the issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered. Enclosed are copies of my records concerning this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\n",
      "\n",
      "Customer service response:\n",
      "Dear Bumblebee, It is your fault. The product is not yours.\n",
      "\n",
      "We will refund the purchase price, in full and in installments.\n",
      "\n",
      "I made a couple of requests, but I ordered one of 2 (you have to pay $5 each) and was disappointed.\n",
      "\n",
      "\n",
      "I sent you a copy of the packaging and requested a refund, but you sent me a shipping address that wasn't\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\")\n",
    "response = \"Dear Bumblebee, It is your fault.\"\n",
    "prompt = text + \"\\n\\nCustomer service response:\\n\" + response\n",
    "outputs = generator(prompt, max_length=200)\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a09caa2f-0f81-44a1-b6af-a8a6c660fee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe7e34b4a8864e6e84beabb4f88953d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d630c38fc3d4e54a77b1f2556ca9dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e2d20f4c8c4faeab0756a19e9d9a51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d4e1ca08db468bbfc33669bda8bee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf4362b649b4e6182edc5f46bd10b80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf12fb1225d44d338e20ced53614542c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff1431771c2c4f578a49a61a03083aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3625773c-765c-469d-b8df-079e28e21a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[8888,  314, 1816,  284,  262, 3952,  290,  220]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Today I went to the park and \"\n",
    "input_ids = tokenizer([input_text], return_tensors='pt')\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0586d9e-e82f-4be1-a617-2da4f21f41a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today I went to the park and  didn't realise this was where he is. We were supposed, that's how long we're going there if you want us back... it should have been a little bit later (or better).\n",
      "I also heard rumours of his father being in bed with him at work today - did anyone know? It has just started yet......<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**input_ids, streamer=streamer, repetition_penalty=1.1, do_sample=True, temperature=1.0, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47ccf02b-32d5-4bdc-9eb1-fac09f5071eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today I went to the park and  I saw a lot of people there. I was really excited to see what they were doing. I was really excited to see what they were doing. I was really excited to see what they were doing. I was really excited to see what they were doing. I was really excited to see what they were doing. I was really excited to see what they were doing. I was really excited to see what they were doing. I was really excited to see\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "output = model.generate(input_ids, max_length=100)\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fda5cef8-22de-4653-a193-909fddc851ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d76cf747327428dbd09cc8efb24300f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad7a4c3d68d6477aa66bb25c1967345a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['perplexities', 'mean_perplexity']\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "input_texts = [\"lorem ipsum\", \"Happy Birthday!\", \"Bienvenue\"]\n",
    "results = perplexity.compute(model_id='gpt2',\n",
    "                             add_start_token=False,\n",
    "                             predictions=input_texts)\n",
    "print(list(results.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78c8259c-a16a-4d69-99a6-b26f172b587d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "646.74\n"
     ]
    }
   ],
   "source": [
    "print(round(results[\"mean_perplexity\"], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88d8618d-67bd-463e-80b6-c7bb560fe07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1499.6920166015625\n"
     ]
    }
   ],
   "source": [
    "print(results[\"perplexities\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd968d0e-cd5d-416d-8ee2-49768df9bf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f80db51-2526-49e6-b583-d688d41743de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1499.6876, grad_fn=<ExpBackward0>)\n"
     ]
    }
   ],
   "source": [
    "inputs_text = tokenizer(\"Happy Birthday!\", return_tensors = \"pt\")\n",
    "loss = model(input_ids = inputs_text[\"input_ids\"], labels = inputs_text[\"input_ids\"]).loss\n",
    "ppl = torch.exp(loss)\n",
    "print(ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a481f2f8-f19a-457b-ac0a-a11d8f5fa27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   262 |  the     | -1.4135 | 24.33%\n",
      "|  1110 |  day     | -2.6088 | 7.36%\n",
      "|   618 |  when    | -2.0095 | 13.41%\n",
      "|   356 |  we      | -1.8593 | 15.58%\n",
      "|   460 |  can     | -2.5082 | 8.14%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "inputs = tokenizer([\"Today is\"], return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=5, return_dict_in_generate=True, output_scores=True)\n",
    "transition_scores = model.compute_transition_scores(\n",
    "    outputs.sequences, outputs.scores, normalize_logits=True\n",
    ")\n",
    "\n",
    "input_length = inputs.input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "    # | token | token string | logits | probability\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.4f} | {np.exp(score.numpy()):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd77b571-8b16-41c6-90fc-c55fa966cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(generated_outputs)\n",
    "# Not sure about what this is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "211c64c6-1423-473f-908a-25eb681e7a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8968a10746044d6aa7c3c0a613201d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b1250b4c944347b90454c87fb13d2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/43.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c596654b8e55462896187a497f43dff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2177b7ec34249fda51c85a12e277809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c3cca71bad4952b7bdd294a2168e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8003a2f5c6544882be770ba4b3595ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf83cda1b25c48f4afb74d826fc9db42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a73f150876cc483c8e06970286f014be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/476 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9088112137943c7b61bb1d1b324b4bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b0a6aac2e494fb0968cce50f5ee63ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f6a652995b41c29fffad9cbe9377e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4227dfef269406884e17cda64c11163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/131 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"KnutJaegersberg/gpt2-chatbot\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"KnutJaegersberg/gpt2-chatbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbbd9327-ee0e-4fad-a201-33f98f0bacf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### System:You are an AI assistant. User will give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.### User: How do you fine tune a large language model? ### Assistant:\n",
      "Fine-tuning a large language model involves fine-tuning the model's parameters, such as the number of hidden layers, the number of hidden units, and the number of hidden units per layer.\n",
      "\n",
      "Here are some steps you can take to fine-tune a large language model:\n",
      "\n",
      "1. Train the model: Train the model on a large dataset of text data. This will help you fine-tune the model's parameters.\n",
      "2. Fine-tune\n"
     ]
    }
   ],
   "source": [
    "input_text = \"### System:\\\n",
    "You are an AI assistant. User will give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.\\\n",
    "### User: \\\n",
    "How do you fine tune a large language model? \\\n",
    "### Assistant:\\\n",
    "\"\n",
    "input_ids = tokenizer([input_text], return_tensors='pt')\n",
    "streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**input_ids, streamer=streamer, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08260f58-b2a7-4885-a5e2-91e3fb8c0e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
