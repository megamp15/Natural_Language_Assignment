{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextStreamer\n",
    "import torch\n",
    "import evaluate\n",
    "from utils import save_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 3 - 8B Instruct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f044080adb5242f18233a853e7914ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, messages, token_limit):\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"\")\n",
    "    ]\n",
    "\n",
    "    hyperparams = {\n",
    "        \"max_new_tokens\": token_limit, \n",
    "        \"temperature\": 0.3, \n",
    "        \"top_p\": 0.9, \n",
    "        \"do_sample\": True\n",
    "    }\n",
    "    streamer = TextStreamer(tokenizer)\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        streamer=streamer, \n",
    "        max_new_tokens=hyperparams[\"max_new_tokens\"],\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=hyperparams[\"do_sample\"],\n",
    "        temperature=hyperparams[\"temperature\"],\n",
    "        top_p=hyperparams[\"top_p\"],\n",
    "    ).to(model.device)\n",
    "\n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "    output = tokenizer.decode(response, skip_special_tokens=True)\n",
    "    save_response(\"LLAMA3-8B-INSTRUCT\", hyperparams, output)\n",
    "    return output\n",
    "\n",
    "def recursion(model, tokenizer, messages, token_limit=1000, MAX_RECURSION_LIMIT=3, recursion_count=0):\n",
    "    output = generate_response(model, tokenizer, messages, token_limit)\n",
    "    if \"[CONTINUE]\" in output.split('\\n')[-1] and recursion_count < MAX_RECURSION_LIMIT:\n",
    "        # Recursive test for unfinished prompts\n",
    "        messages.append({\"role\": \"system\", \"content\": f\"\"\"{output}\"\"\"})\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"\"\"Can you continue the story at the last sentence before \"[CONTINUE]\". Again you do not need to use the max new token limit but if you need more tokens to complete the story then write the following at the end of the new output: \"[CONTINUE]\" \"\"\"})\n",
    "        return recursion(model, tokenizer, messages, token_limit, MAX_RECURSION_LIMIT, recursion_count + 1)\n",
    "    else:\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "{\"role\": \"system\", \"content\": f\"\"\"You are a AI writer who creates captivating and motivating stories in various genres. \n",
    "You are writing stories for a mobile application that helps users walk/run or train for marathons. \n",
    "The genre and description for the story will come by user input called Prompt.\n",
    "The stories are meant to be played during a training session in order to help the user train for marathons in an engaging and motivated manner. \n",
    "The story should have embedded cues for speed changes and breaks for a duration of distance or time where approriate depending on the Training Style.\n",
    "\n",
    "Here's a template of how you should write the story:\n",
    "\n",
    "Story Title\n",
    "\n",
    "An intro and a starting cue of how to begin the training given the training style\n",
    "\n",
    "Cue: [Speed up/down for XX miles/minutes] or [Take a break for XX minutes] or [Sprint/Walk/Run for XX miles/minutes] or [Sprint/Walk/Run to the finish]. It can be something similar to this. Just keep it consistent. \n",
    "\n",
    "Write XX amount of sentences that matches the length of the cue for the story\n",
    "\n",
    "Cue: [Speed up/down for XX miles/minutes] or [Take a break for XX minutes] or [Sprint for XX miles/minutes] or [Walk for XX miles/minutes]\n",
    "\n",
    "Write XX amount of sentences that matches the length of the cue for the story\n",
    "\n",
    "Continue this pattern of Cue and writing the next section of sentences until you've reached the distance for the sesssion. \n",
    "\n",
    "Cue: [Walk/Sprint/Run to the finish for XX miles/minutes]\n",
    "\n",
    "End the story with XX sentences for the length of the last cue\n",
    "\n",
    "An example is below for the given user inputs:\n",
    "\n",
    "Can you help me write a story with the following details:\n",
    "Prompt: A zombie horror story set in New York City.\n",
    "Distance: 0.5 miles\n",
    "Average Walking Speed: 2 mph\n",
    "Average Running Speed: 5 mph\n",
    "Training Style: Short-distance sprint\n",
    "\n",
    "Output: \n",
    "Here's a zombie horror story set in New York City for a 0.5-mile training session with a short-distance sprint training style.\n",
    "\n",
    "**Story: \"The Brooklyn Bridge Apocalypse\"**\n",
    "\n",
    "You're a survivor of a zombie outbreak in New York City. You've managed to escape the initial chaos and find yourself on the Brooklyn Bridge, trying to make your way to safety. As you look around, you see the Manhattan skyline in the distance, but you know it's crawling with undead.\n",
    "\n",
    "You start your journey, your heart racing with every step. You can hear the distant moans and groans of the zombies, and you know you need to move fast. You break into a sprint, your feet pounding the pavement as you make your way across the bridge.\n",
    "\n",
    "**Cue: Speed up to 5 mph**\n",
    "\n",
    "As you run, you notice the zombies are getting closer. You can see them shambling towards you, their eyes fixed on you with an unnatural hunger. You pick up speed, your breathing heavy as you try to outrun the undead.\n",
    "\n",
    "Suddenly, you hear a loud thud behind you. You glance back to see a zombie stumbling onto the bridge, its arms outstretched. You don't hesitate, pushing yourself to run even faster.\n",
    "\n",
    "**Cue: Sprint for 0.25 miles**\n",
    "\n",
    "You're almost halfway across the bridge when you see a group of survivors huddled together, trying to escape the chaos. You spot a safe haven and make a beeline for it.\n",
    "\n",
    "As you approach, you notice the survivors are armed and ready to defend themselves. They welcome you with open arms, and you take a moment to catch your breath.\n",
    "\n",
    "**Cue: Walk for 0.15 miles**\n",
    "\n",
    "But your respite is short-lived. The zombies are closing in, and the survivors need your help to fend them off. You grab a nearby bat and join the fight, swinging it wildly at the undead.\n",
    "\n",
    "**Cue: Sprint for 0.1 miles**\n",
    "\n",
    "You manage to fend off the zombies, but you know you need to keep moving. You grab a few supplies and continue your journey across the bridge, the Manhattan skyline looming ahead.\n",
    "\n",
    "As you near the end of the bridge, you see a glimmer of hope â€“ a safe zone, marked by a bright light. You sprint towards it, the zombies hot on your heels.\n",
    "\n",
    "**Cue: Sprint to the finish**\n",
    "\n",
    "You burst through the gates, exhausted but alive. You've made it to safety, but you know the fight is far from over.\n",
    "\n",
    "This example is good but there is not enough sentences in between the cues for the distance specified. \n",
    "\n",
    "You do not need to use the max new token limit to complete the story. if you need more tokens to complete the story, then write the following at the end of the output: \"[CONTINUE]\". If the story is complete, do not write \"[CONTINUE]\" at the end of the output.\"\"\"}, \n",
    "{\"role\": \"user\", \"content\": f\"\"\"Prompt: Embark on an action-packed adventure inspired by Indiana Jones, set in the heart of a dense, mysterious jungle. The protagonist, a daring archaeologist, must navigate treacherous terrain, evade deadly traps, and outwit a cunning rival in the quest for an ancient artifact. The story should include intense sprinting segments as the protagonist narrowly escapes landslides and dodges deadly traps, steady jogs while following hidden paths marked by ancient symbols, moments of high-speed chases through jungle underbrush with the rival treasure hunter hot on their heels, and brief periods of rest in hidden safe spots to catch their breath before the next challenge.\n",
    "Distance: 5 miles\n",
    "Average Walking Speed: 2.75 mph\n",
    "Average Running Speed: 5 mph\n",
    "Training Style: Long-Distance Walk\"\"\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrecursion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 36\u001b[0m, in \u001b[0;36mrecursion\u001b[1;34m(model, tokenizer, messages, token_limit, MAX_RECURSION_LIMIT, recursion_count)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecursion\u001b[39m(model, tokenizer, messages, token_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, MAX_RECURSION_LIMIT\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, recursion_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m---> 36\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_limit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[CONTINUE]\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m recursion_count \u001b[38;5;241m<\u001b[39m MAX_RECURSION_LIMIT:\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;66;03m# Recursive test for unfinished prompts\u001b[39;00m\n\u001b[0;32m     39\u001b[0m         messages\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\"\"\u001b[39m})\n",
      "Cell \u001b[1;32mIn[10], line 20\u001b[0m, in \u001b[0;36mgenerate_response\u001b[1;34m(model, tokenizer, messages, token_limit)\u001b[0m\n\u001b[0;32m     13\u001b[0m hyperparams \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_new_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: token_limit, \n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.3\u001b[39m, \n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.9\u001b[39m, \n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_sample\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     18\u001b[0m }\n\u001b[0;32m     19\u001b[0m streamer \u001b[38;5;241m=\u001b[39m TextStreamer(tokenizer)\n\u001b[1;32m---> 20\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_new_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mterminators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdo_sample\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     30\u001b[0m response \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m][input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:]\n\u001b[0;32m     31\u001b[0m output \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(response, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\pmahi\\OneDrive\\Desktop\\MP\\Drexel\\Courses\\CS614-900 - Applications in Machine Learning\\Projects\\Natural_Language_Assignment\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pmahi\\OneDrive\\Desktop\\MP\\Drexel\\Courses\\CS614-900 - Applications in Machine Learning\\Projects\\Natural_Language_Assignment\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1565\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1562\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m inputs_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1564\u001b[0m device \u001b[38;5;241m=\u001b[39m inputs_tensor\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m-> 1565\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_special_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs_has_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1567\u001b[0m \u001b[38;5;66;03m# decoder-only models must use left-padding for batched generation.\u001b[39;00m\n\u001b[0;32m   1568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m   1569\u001b[0m     \u001b[38;5;66;03m# If `input_ids` was given, check if the last id in any sequence is `pad_token_id`\u001b[39;00m\n\u001b[0;32m   1570\u001b[0m     \u001b[38;5;66;03m# Note: If using, `inputs_embeds` this check does not work, because we want to be more hands-off.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pmahi\\OneDrive\\Desktop\\MP\\Drexel\\Courses\\CS614-900 - Applications in Machine Learning\\Projects\\Natural_Language_Assignment\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1405\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_special_tokens\u001b[1;34m(self, generation_config, kwargs_has_attention_mask, device)\u001b[0m\n\u001b[0;32m   1400\u001b[0m     generation_config\u001b[38;5;241m.\u001b[39mdecoder_start_token_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_decoder_start_token_id(\n\u001b[0;32m   1401\u001b[0m         generation_config\u001b[38;5;241m.\u001b[39mdecoder_start_token_id, generation_config\u001b[38;5;241m.\u001b[39mbos_token_id\n\u001b[0;32m   1402\u001b[0m     )\n\u001b[0;32m   1404\u001b[0m bos_token_id \u001b[38;5;241m=\u001b[39m _tensor_or_none(generation_config\u001b[38;5;241m.\u001b[39mbos_token_id, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m-> 1405\u001b[0m eos_token_id \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_or_none\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1406\u001b[0m pad_token_id \u001b[38;5;241m=\u001b[39m _tensor_or_none(generation_config\u001b[38;5;241m.\u001b[39mpad_token_id, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m   1407\u001b[0m decoder_start_token_id \u001b[38;5;241m=\u001b[39m _tensor_or_none(generation_config\u001b[38;5;241m.\u001b[39mdecoder_start_token_id, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\pmahi\\OneDrive\\Desktop\\MP\\Drexel\\Courses\\CS614-900 - Applications in Machine Learning\\Projects\\Natural_Language_Assignment\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1396\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_special_tokens.<locals>._tensor_or_none\u001b[1;34m(token, device)\u001b[0m\n\u001b[0;32m   1394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m   1395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m token\n\u001b[1;32m-> 1396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "recursion(model, tokenizer, messages, 2000, 3, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
